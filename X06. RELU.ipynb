{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Relu is used in the middle layer of the network to regularize the activation, in other words activation should not be in nalegative either it should be zero or more than that. The relu layer duty is to check the activation if output greater than or equal to zero will leave it as it is else convert it to zero it. Every activation will be from 0 to +n not negative value.\n",
    "\n",
    "Softmax is used for the output layer in multi class classification problems. It outputs a vector of probabilities of each class. If you're doing binary classification, you could use sigmoid for the output layer as well(logistic regression basically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, -1.5, 1.5, 2.5, -3.5, -0.1]\n",
      "[0.  0.  1.5 2.5 0.  0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ds/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#The negative values in the vector are replaced with zero. We changed a linear data set to a non-linear one.\n",
    "#Relu is a transformation that adds non-linearity.\n",
    "import tensorflow as tf\n",
    "\n",
    "vector = [0., -1.5, 1.5, 2.5, -3.5, -0.1]\n",
    "\n",
    "# Use relu to convert to linear data by replacing negative values with 0.\n",
    "r = tf.nn.relu(vector)\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "# Print the vector and the relu result.\n",
    "print(vector)\n",
    "print(session.run(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "# our input data\n",
    "# None means we don't know the number of rows yet\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2]) \n",
    " \n",
    "# the hidden layer\n",
    "wh = tf.Variable(tf.random_normal([2, 2]))\n",
    "bh = tf.Variable(tf.random_normal([2]))\n",
    "# Our input data\n",
    "input = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "output = [[0], [1], [1], [0]]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
