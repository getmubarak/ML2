{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S Let/VB 's/POS meet/VB tomorrow/NN at/IN 9/CD pm/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"Let's meet tomorrow at 9 pm\";\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print (nltk.ne_chunk(pos_tags, binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (NE New/NNP York/NNP)\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  (NE John/NNP Smith/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I went to New York to meet John Smith\";\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print (nltk.ne_chunk(pos_tags, binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Andrew', 'PERSON'), ('Chinese', 'GPE'), ('American', 'GPE'), ('Baidu', 'ORGANIZATION'), (\"company's Artificial Intelligence Group\", 'ORGANIZATION'), ('Stanford University', 'ORGANIZATION'), ('Coursera', 'ORGANIZATION'), ('Andrew', 'PERSON'), ('Hong Kong', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "doc = '''Andrew Yan-Tak Ng is a Chinese American computer scientist.\n",
    "He is the former chief scientist at Baidu, where he led the company's\n",
    "Artificial Intelligence Group. He is an adjunct professor (formerly \n",
    "associate professor) at Stanford University. Ng is also the co-founder\n",
    "and chairman at Coursera, an online education platform. Andrew was born\n",
    "in the UK in 1976. His parents were both from Hong Kong.'''\n",
    "# tokenize doc\n",
    "tokenized_doc = nltk.word_tokenize(doc)\n",
    " \n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    " \n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "        entity_type = tagged_tree.label() # get NE category\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The NLTK's NEC works by using a supervised machine learning algorithm known as a MaxEnt classifier. A MaxEnt classifier gets its name from maximum entropy. For a discrete probability distribution, maximum entropy is obtained when the distribution is uniform. A MaxEnt classifier is logistic regression. The difference is theoretical, because in the MaxEnt derivation, you assume maximum entropy and derive the sigmoid function. In the logistic regression derivation, you assume the sigmoid function. [J. Mount].\n",
    "\n",
    "This machine learning model uses data from a corpus that has been manually annotated for NEs. A person, called an annotator, will read sentence after sentence and manually mark where the NEs are found in text. This is of course, a very tedious task. It is no wonder that most annotated corpora are not distributed for free. In fact, the NLTK does not provide you with the corpora it trained the NEC on (it was trained on data from ACE--Automatic Content Extraction). What the authors did provide, however, was a pickle file (a python serialized object) trained on this data. This pickle file, is a freeze-dried instance of the statistics needed for the MaxEnt classifier.\n",
    "\n",
    "A note I'd like to add, is that the NLTK does provide NE annotated data found in corpora/ieer. However, unless that data is a good representation of the data you want to classify on, I wouldn't recommend using it. Also, you will have to write your own feature extractor for this, because the format in IEER is different than ACE."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " So what features are used in NLTK's NEC? I've listed them below:\n",
    "The shape of the word (e.g., does it contain numbers? does it begin with a capital letter?)\n",
    "The length of the word\n",
    "The first three letters of the word\n",
    "The last three letters of the word\n",
    "The POS tag of the word\n",
    "The word itself\n",
    "Does the word exist in an English dictionary?\n",
    "The tag of the word that precedes this word (i.e., was the previous word identified as a NE)\n",
    "The POS tag of the preceding word\n",
    "The POS tag of the following word\n",
    "The word that precedes this word\n",
    "The word that follows this word\n",
    "The word combined with the POS tag of the following word\n",
    "The POS tag of the word combined with the tag of the preceding word\n",
    "The shape of the word combined with the tag of the preceding word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ". Essentially, NER is a\n",
    "way of extracting some of the most common entities, such as names, organizations,\n",
    "and locations. However, some of the modified NER can be used to extract entities\n",
    "such as product names, biomedical entities, author names, brand names, and so on\n",
    "\n",
    "\n",
    "GPE stands for \"Geo-political entity\", that is, a location\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
